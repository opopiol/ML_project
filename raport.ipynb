{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "raport.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlRyb5PS8o2YyJ1Icy+mmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opopiol/ML_project/blob/model/raport.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6BvWinAcIv1"
      },
      "source": [
        "### **1. EDA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAWIVIuCci8_"
      },
      "source": [
        "After checking\n",
        "*   there is no problem with missing values\n",
        "*   the data is almost normally distributed\n",
        "\n",
        "We also change -1 to 0, because it was easier to visualize data later.\n",
        "\n",
        "There have been loaded two scalers `StandardScaler()` and `MinMaxScaler()`, but we will be using only `StandardScaler()` later. Data scaling is a recommended pre-processing step when working with many machine learning algorithms.\n",
        "\n",
        "It is used in scatterplot, which started a tries of proper clustering. We are using `PCA()` to reduce the dimension of features. But none of the use of the PCA itself has worked out.\n",
        "\n",
        "Later the first pipeline was created. It included apart from `PCA()` also `TSNE()`. It turns out that best for our project will be n_components=0.99 for PCA and n_components=2 for TSNE, because we can clearly see how the data grouped in unsupervised way. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prEIozbJlYjI"
      },
      "source": [
        "For splitting data we use: `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y, random_state=1)`. Splitted dataset: 70% is used to train the model and 30% to evaluate it. `stratify=y` will keep proportion of values in the sample produced, the same as the proportion of values provided to parameter stratify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10zp9NPrcS5F"
      },
      "source": [
        "### **2. Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S84oo6uJCbRA",
        "outputId": "2473be29-919d-4224-fbca-41c93414fe62"
      },
      "source": [
        "y.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "y \n",
              " 1    3375\n",
              "-1     375\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrfdiTFcmCq"
      },
      "source": [
        "The distribution of the labels is very uneven. They appear in proportions of about 1:10.\n",
        "\n",
        "Thatâ€™s why we decided to choose f1-score because it fits the best to our dataset and deals best with disproportion. This metric is the harmonic mean of the both precision and recall.\n",
        "\n",
        "And because we have to predict the class of given data we use classifiers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmQxzc7scXKq"
      },
      "source": [
        "### **3. Baseline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmz0L3A6e64Q"
      },
      "source": [
        "For the base model we choose `DummyClassifier()`, beacuase it is a simple baseline for the other classifiers. \n",
        "\n",
        "Strategies we used are `['stratified', 'most_frequent', 'constant', 'uniform']` and we get the following results:\n",
        "\n",
        "*   tratified strategy f1 score is 0.8951813214108296\n",
        "*   most_frequent strategy f1 score is 0.9476145930776426\n",
        "*   prior strategy f1 score is 0.9476145930776426\n",
        "*   uniform strategy f1 score is 0.6650093225605965\n",
        "\n",
        "\n",
        "The highest results were achived by `most_frequent` strategy and `prior` strategy with a result above 94%. But both of them were only using one, majority class.\n",
        "\n",
        "For another baseline, we decided to check also `DecisionTreeClassifier()` and its f1 score was above 96%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9I2AtyTcZsB"
      },
      "source": [
        "### **4. Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxoT9a4jcZxe"
      },
      "source": [
        "In the first approach to choosing the best class factor we decided to check the following three classifiers `LogisticRegression(random_state=0, max_iter = 1000)`, `KNeighborsClassifier()` and `SVC()`.\n",
        "\n",
        "Their f1 scores were:\n",
        "\n",
        "*   `LogisticRegression` 0.891662506240639\n",
        "*   `KNeighborsClassifier` 0.9803921568627452\n",
        "*   `SVC` 0.9767211490837047\n",
        "\n",
        "Because `LogisticRegression` achieved a result lower than the base, it was not taken into account further.\n",
        "\n",
        "`KNeighborsClassifier`, with its default number of 5 neighbors, performed the best out of three given classifiers. Also model is usually robust to noisy data.\n",
        "\n",
        "`SVC` also performed better f1 score than our base model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0ymzjfe_sWc"
      },
      "source": [
        "Later we were looking for more advanced calssifiers. Some of the most popular are AdaBoost, Gradient Boosting, and XGBoost. Due to the long processing time, we decided to try only one- `AdaBoostClassifier()` to see what results it would achieve. In its basic model, without the given parameters, it achieved a f1 score 0.9656188605108056, so above baseline score.\n",
        "\n",
        "> In its basic model, without the given parameters, it achieved a f1 score 0.9656188605108056, so above baseline score.\n",
        "\n",
        "Then a pipeline was created to see which parameters of `AdaBoostClassifier()` performed best. In the pipeline were used: `'n_estimators': [50, 100, 150], 'learning_rate': [0.1, 0.01, 0.05]`. It was done in order not to overload the fan pipeline so much. \n",
        "\n",
        "> It turns out that the best parameters was `algorithm='SAMME.R', base_estimator=None, learning_rate=0.05, n_estimators=50, random_state=None`. The model achieved a score of 0.9793510324483775.\n",
        "\n",
        "These params were used in final pipeline with others classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7jq40XPDi4q"
      },
      "source": [
        "The final pipeline had parameters and models choosen earlier.\n",
        "\n",
        "`PCA()` got n_components = 0.99, because with this params it performed the best in EDA's clustering.\n",
        "As scaler we got `StandardScaler()` same as in EDA and does not requires any parameters to be optimised by GridSearchCV. \n",
        "\n",
        "In pipeline three calssifier were checked: \n",
        "\n",
        "*   `SVC()` with `kernel: ['linear', 'poly'], class_weight: ['balanced'], and C: np.logspace(1,4,5)`\n",
        "*   `KNeighborsClassifier()` with `n_neighbors': [2, 4, 6, 8, 10], algorithm': ['auto']`\n",
        "*   `AdaBoostClassifier()` with the parameters that came out in the previous pipeline `n_estimators': [50], learning_rate': [0.1]`\n",
        "\n",
        "We used GridSearchCV to select the best model. To avoid overfitting, instead of the data that was used for training, we used `test_data`.\n",
        "\n",
        "\n",
        "> The result of `best_model.best_estimator_` was `'classifier', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.1, n_estimators=50, random_state=None)`. \n",
        "Again `AdaBoostClassifier` was choosed as the best model and this is our final choice. "
      ]
    }
  ]
}